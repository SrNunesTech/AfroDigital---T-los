

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


#Importar as bases de dados
vendas_2023 = pd.read_excel('/content/Vendas_de_2023.xlsx')
total_de_vendas = pd.read_excel('/content/Total_de_vendas.xlsx')
satisfacao_dos_clientes = pd.read_excel('/content/Satisfacao_dos_Clientes.xlsx')
pesquisa_populacao = pd.read_excel('/content/Pesquisa_Populacao_Geral.xlsx')
#1. Identificar a loja com a maior média de vendas em 2023.

# Excluir a coluna "Cidade" para calcular a média das vendas
media_vendas = vendas_2023.iloc[:, 1:].mean(axis=1)

# Adicionar a média de vendas como uma nova coluna
vendas_2023['Media_Vendas'] = media_vendas

# Exibir o DataFrame com as médias de vendas
print(vendas_2023[['Cidade', 'Media_Vendas']])

# Identificar a cidade com a maior média de vendas
cidade_maior_media = vendas_2023.loc[vendas_2023['Media_Vendas'].idxmax(), 'Cidade']

# Exibir a cidade com a maior média de vendas
print(f"Cidade com a maior média de vendas em 2023: {cidade_maior_media}")

#2.Criar um gráfico de barras com o ranking das lojas baseado na média de vendas.
# Excluir a coluna "Cidade" para calcular a média das vendas
media_vendas = vendas_2023.iloc[:, 1:].mean(axis=1)

# Adicionar a média de vendas como uma nova coluna
vendas_2023['Media_Vendas'] = media_vendas
# Criar o gráfico de barras
plt.figure(figsize=(10, 6))
plt.barh(vendas_2023['Cidade'], vendas_2023['Media_Vendas'], color= 'skyblue')

#Add Titulos ao gráfico
plt.xlabel('Média de Vendas')
plt.ylabel('Lojas')
plt.title('Ranking das Médias de Vendas por Loja')
#Ordenando Gráfico(Decrescente)
plt.gca().invert_yaxis()
plt.show()

#3. Analisar a pesquisa de mercado para determinar quais lojas são mais conhecidas em suas respectivas cidades.

# Carregar a planilha Excel
df = pd.read_excel('Pesquisa_Populacao_Geral.xlsx')

# Exibir as primeiras linhas do DataFrame para entender a estrutura
print(df.head())
df.columns = df.columns.str.strip()

# Calcular a moda das respostas para cada loja em cada cidade
moda_respostas = df.groupby(['Cidade'])['Resposta'].agg(lambda x: x.mode().iloc[0])

# Exibir a moda das respostas
print(moda_respostas)

# Determinar quais lojas são conhecidas
# Uma loja é considerada conhecida se a moda das respostas for "Sim"
moda_respostas_df = moda_respostas.reset_index()
moda_respostas_df['Conhecida'] = moda_respostas_df['Resposta'] == 'Sim'

# Exibir os resultados
print(moda_respostas_df)

# Visualizar os resultados (opcional)
# Criar um DataFrame para lojas conhecidas
lojas_conhecidas = moda_respostas_df[moda_respostas_df['Conhecida']]

# Criar um DataFrame para lojas não conhecidas
lojas_nao_conhecidas = moda_respostas_df[~moda_respostas_df['Conhecida']]

print("\nLojas Conhecidas:")
print(lojas_conhecidas)

print("\nLojas Não Conhecidas:")
print(lojas_nao_conhecidas)

#4. Listar as lojas que estão no quartil com as piores médias de vendas de 2023.
file_path = 'Vendas_de_2023.xlsx'
vendas_2023 = pd.read_excel(file_path)

# Excluir a coluna "Cidade" para calcular a média das vendas
vendas = vendas_2023.iloc[:, 1:]  # Ajuste se necessário

# Calcular a média das vendas
vendas_2023['Media_Vendas'] = vendas.mean(axis=1)

# Calcular o desvio padrão das vendas
vendas_2023['Desvio_Padrao'] = vendas.std(axis=1)

# Exibir o DataFrame com as médias e desvios padrão de vendas
print(vendas_2023[['Cidade', 'Media_Vendas', 'Desvio_Padrao']])

# Calcular os quartis das médias de vendas
quartis = vendas_2023['Media_Vendas'].quantile([0.25, 0.5, 0.75])
print("Quartis das médias de vendas:", quartis)

# Determinar o valor do 1º quartil (Q1)
q1 = quartis[0.25]

# Listar as lojas que estão no quartil inferior (com média de vendas <= Q1)
lojas_quartil_inferior = vendas_2023[vendas_2023['Media_Vendas'] <= q1]

# Exibir as lojas no quartil inferior
print("\nLojas no quartil inferior (pior 25% de médias de vendas):")
print(lojas_quartil_inferior[['Cidade', 'Media_Vendas', 'Desvio_Padrao']])
#5. Determinar qual loja do pior quartil é a mais inconstante em suas vendas.
# Encontrar a loja mais inconstante (com maior desvio padrão) no pior quartil
loja_mais_inconstante = lojas_quartil_inferior.loc[lojas_quartil_inferior['Desvio_Padrao'].idxmax()]

# Exibir a loja mais inconstante no pior quartil
print("\nA loja mais inconstante no pior quartil é:")
print(loja_mais_inconstante[['Cidade', 'Media_Vendas', 'Desvio_Padrao']])

#6 De acordo com os dados temporais apresentados, como serão as vendas dos próximos 3 anos para a região nordeste da empresa?

# Carregar a planilha Excel
file_path = 'Vendas_de_2023.xlsx'
df = pd.read_excel(file_path)

# Lista de cidades do Nordeste
cidades_nordeste = [
    'Salvador', 'Fortaleza', 'Recife', 'São Luís', 'Maceió', 'Natal', 'Teresina',
    'João Pessoa', 'Aracaju', 'Vitória da Conquista', 'Caucaia', 'Campina Grande',
    'Petrolina', 'Caruaru', 'Mossoró', 'Feira de Santana', 'Juazeiro do Norte',
    'Ilhéus', 'Arapiraca', 'Barreiras', 'Paulo Afonso', 'Camaçari', 'Sobral',
    'Itabuna', 'Parnaíba', 'Ipojuca', 'Garanhuns', 'Timon', 'Santa Cruz do Capibaribe',
    'Patos', 'Teixeira de Freitas', 'Jequié', 'Crato', 'Lagarto', 'Itapipoca',
    'Quixadá', 'Picos', 'Açailândia', 'Bacabal', 'Caxias', 'Codó', 'Piripiri',
    'Barbalha', 'Russas', 'Simões Filho', 'Juazeiro', 'Eunápolis', 'Jacobina',
    'Salgueiro', 'Arcoverde', 'Pesqueira', 'Alagoinhas', 'Itaberaba', 'Senhor do Bonfim',
    'Morro do Chapéu', 'Irecê', 'Serra Talhada', 'Ouricuri', 'Parnamirim',
    'Caicó', 'Santa Rita', 'Guarabira', 'Bayeux', 'Cabedelo', 'Catolé do Rocha',
    'Sousa', 'Nazária', 'Floriano', 'Piripiri', 'Oeiras', 'Picos', 'Parnaíba',
    'Barrinha', 'Catu', 'Eusébio', 'Itaitinga', 'Horizonte', 'Pacajus', 'Maranguape',
    'Cascavel', 'Aquiraz', 'Limoeiro do Norte', 'Tianguá', 'Crateús', 'Canindé',
    'Iguatu', 'Quixeramobim', 'Itapajé', 'Açu', 'Currais Novos', 'Apodi', 'Canguaretama',
    'Macau', 'Nova Cruz', 'Cruz das Almas', 'Santo Antônio de Jesus', 'Valença',
    'Ipiaú', 'Itamaraju', 'Itapetinga', 'Itaberaba', 'Senhor do Bonfim', 'Cruz das Almas',
    'Santo Antônio de Jesus', 'Valença', 'Ipiaú', 'Itamaraju', 'Itapetinga',
    'Santa Inês', 'Pinheiro', 'Pedreiras', 'Coroatá', 'Barra do Corda', 'Grajaú',
    'Viana', 'Coelho Neto', 'Chapadinha', 'Santa Luzia', 'Presidente Dutra', 'Buriticupu',
    'Santa Helena', 'São Mateus do Maranhão', 'Icatu', 'Tutóia', 'Urbano Santos',
    'Cururupu', 'Matinha', 'Peritoró', 'Zé Doca', 'Rosário', 'Miranda do Norte',
    'São João dos Patos', 'Esperantinópolis', 'Bequimão', 'Vargem Grande', 'Guimarães',
    'Barreirinhas', 'Arari', 'Cantanhêde', 'Amarante do Maranhão', 'Carutapera',
    'Brejo', 'Anapurus', 'Santa Quitéria do Maranhão', 'Humberto de Campos', 'Belágua',
    'Primeira Cruz', 'Paulino Neves', 'Tutóia', 'Timbiras', 'Capinzal do Norte',
    'Bernardo do Mearim', 'Bom Jardim', 'Bom Jesus das Selvas', 'Buriti', 'São Benedito do Rio Preto'
]

# Filtrar as cidades da região Nordeste
df_nordeste = df[df['Cidade'].isin(cidades_nordeste)]

# Transformar os dados em um formato de série temporal
# Somar as vendas de todas as cidades do Nordeste para cada mês
vendas_mensais = df_nordeste.iloc[:, 1:].sum()

# Criar um índice de datas para os meses de 2023
dates = pd.date_range(start='2023-01-01', periods=12, freq='M')
vendas_mensais.index = dates

# Verificar se os dados estão no formato correto
print(vendas_mensais)

# Criar o modelo de suavização exponencial (Holt-Winters) sem sazonalidade
model = ExponentialSmoothing(vendas_mensais, trend='add')
fit = model.fit()

# Fazer previsões para os próximos 3 anos (36 meses)
forecast = fit.forecast(36)

# Plotar os resultados
plt.figure(figsize=(12, 6))
plt.plot(vendas_mensais, label='Vendas Mensais (2023)')
plt.plot(forecast, label='Previsão (2024-2026)', linestyle='--')
plt.xlabel('Data')
plt.ylabel('Vendas')
plt.title('Previsão de Vendas para a Região Nordeste (2024-2026)')
plt.legend()
plt.show()

#7 Utilizar um modelo de regressão linear para prever a probabilidade de um cliente voltar a fazer negócio com base no nível de satisfação
# Carregar a planilha Excel
file_path = 'Satisfacao_dos_Clientes.xlsx'
df = pd.read_excel(file_path)

# Verificar a estrutura dos dados
print(df.head())

# Remover linhas com valores NaN
df = df.dropna(subset=['Satisfação dos clientes (1 - 100)', 'Probabilidade do cliente retornar (0% - 100%)'])

# Separar as variáveis independentes (X) e dependentes (y)
X = df[['Satisfação dos clientes (1 - 100)']]
y = df['Probabilidade do cliente retornar (0% - 100%)']

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Criar o modelo de regressão linear
model = LinearRegression()

# Treinar o modelo com os dados de treinamento
model.fit(X_train, y_train)

# Fazer previsões com os dados de teste
y_pred = model.predict(X_test)

# Avaliar o modelo
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

# Plotar os resultados
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='blue', label='Dados Reais')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Previsões')
plt.xlabel('Satisfação dos clientes (1 - 100)')
plt.ylabel('Probabilidade do cliente retornar (0% - 100%)')
plt.title('Regressão Linear: Satisfação dos Clientes vs. Probabilidade de Retorno')
plt.legend()
plt.show()

# Exibir os coeficientes da regressão linear
print(f'Coeficiente: {model.coef_[0]}')
print(f'Intercepto: {model.intercept_}')


     
